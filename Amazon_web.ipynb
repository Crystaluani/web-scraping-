{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jHKljQ9-bbCK"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import smtplib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to the website\n",
        "URL = 'https://www.amazon.com/funny-analyst-definition-scientist-t-shirt/dp/B07NLP2PKY/ref=sr_1_2?crid=1ASBRE02BW3AM&dib=eyJ2IjoiMSJ9.RyYFpfrBS-xIY5xBoSTFLtWgIHIo8W6ZHUtpoBsTD_m4lBoeILKZ4u3SoA5FcyNZ5-jNCsPvjAnuH5It6YyWn8NYsV3Qsiik6eirYDuBFKAA0p9sxn39oTlG3av6tWvcfzu7_GG3CRXCfTd3b8u7grnJYpj26F6twbs9OGeTxAmdN4jakbXm6vHE4x_jrpUyMIxkY1dPyElZBsvENE860-7b7anMyER83WYbZUSWYnKYig9fAnbTtVho8OuuzQVsFIV576ib7g_119K4PG_7VA7qaV3KjkZZSU5YnH98ZAnI7U3VSAjF2fyQxXdnNT5o49D5Wld55NSuP07uD5iy1X7-0dr63bNKD16QnRWRnr_SoLn5sQ15kgDZLRW_ho-AJjD_lIoT-ARYRpfJY5bz6VzopIg-aQkFZ7ZPHT_RLdY_rQsIyC4kB7GeMlccdauL.Sz6Wro0jQn6QhfYSxJhjuq9tODgkLKXL6vnfPiZr6Cc&dib_tag=se&keywords=data+analyst+tshirt&qid=1735411120&sprefix=data+analyst+tshir%2Caps%2C336&sr=8-2'\n",
        "\n",
        "headers ={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0\"}\n",
        "\n",
        "page = requests.get(URL, headers=headers)\n",
        "\n",
        "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
        "\n",
        "\n",
        "title = soup2.find(id='productTitle').get_text()\n",
        "\n",
        "price = soup2.find(id='priceblock_ourprice').get_text()\n",
        "\n",
        "\n",
        "print(title)\n",
        "print(price)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "78n_39zTccuW",
        "outputId": "cc3586c3-37d9-4f28-89d1-ecbefd9df799"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-199d1ceb9623>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'productTitle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'priceblock_ourprice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_tag = soup2.find(id=\"productTitle\")\n",
        "title = title_tag.get_text().strip() if title_tag else \"Title not found\"\n",
        "print(title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2ivKZIrtio4",
        "outputId": "e599d5df-d124-4bff-9e1d-d4879948cfb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Project Title and Goal\n",
        " Amazon Product Data Scraper\n",
        "\n",
        "The goal of this project was to scrape product details (titles, prices, and ratings) from Amazon to build a dataset for analysis.\n",
        "\n",
        "#Tools and Technologies Used\n",
        "Python\n",
        "Libraries: requests, datetime, time , smtplib, BeautifulSoup and pandas\n",
        "Any additional tools (google collab and browser.)\n",
        "\n",
        "#Approach\n",
        "Sent HTTP requests to Amazon using requests.\n",
        "Parsed the HTML with BeautifulSoup.\n",
        "Targeted elements like product titles and prices using their IDs and classes.\n",
        "Handled missing elements and used error handling to prevent crashes.\n",
        "# Challenge\n",
        "Frequent changes in Amazonâ€™s HTML structure.\n",
        "Missing elements in the scraped HTML due to bot detection.\n",
        "# Lessons Learned\n",
        "I gained valuable insights into web scraping techniques and the importance of adapting to website-specific challenges. I also learned about legal and ethical considerations in web scraping.\n",
        "\n",
        "# Outcomes\n",
        "Explain the results:\n",
        "Although the project faced technical limitations due to Amazon's anti-scraping measures, it provided me with hands-on experience in working with web scraping libraries and debugging complex issues.\n",
        "\n",
        "# how i tried to overcome the obstale\n",
        "Printing and inspecting the HTML to understand missing elements.\n",
        "\n",
        "Considering Selenium for dynamic content."
      ],
      "metadata": {
        "id": "v2XRcy_nrz5R"
      }
    }
  ]
}